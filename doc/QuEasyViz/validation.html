<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>QPipeViz Documentation: Model Validation</title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
</head>
<body>

<table width="700" border="0" align="center" cellpadding="0"> <tr> <td>
	
<h2 align="center">
<a href="featureSelection.html"><img src="images/left.png" width="58" height="28" border=0 align="MIDDLE"></a>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="index.html">Index</a> &nbsp;&nbsp;&nbsp;&nbsp;
<a href="executePipeline.html"><img src="images/right.png" width="58" height="28" border=0 align="MIDDLE"></a>
</h2><hr width=700>

<!-- end of header -->	


	
<H2><div align="center">Model Validation</div></H2>
	<TABLE width="700">
   		<tr>	
		<TD width="430" colspan="1" align="center"><IMG src="images/val1_scaled.png"></TD>
		<TD width="270" colspan="1" valign="top"><p>In order to create a new model validation selection, just drag its item from the validation selection panel onto an existing model item within the pipeline area (a).</p>
		<p>A dialog requesting the necessary parameters will appear then. The parameters for the different model validation techniques will be explained below.</p>
		</tr>	
 	</TABLE>



	<H3><div align="center" id="cv">Cross Validation</div></H3>
		<p>Cross Validation divides the data into k partitions and during k iterations always retrains the model on (k-1) of those partitions and uses the remaining one to evaluate the predictive quality. This is done in such a way that in total each partition is used exactly once as test set.</p>
		<p>The average of the k folds is then used to describe the predicitve quality of the model.</p>
		The number of desired folds can be set in the dialog that shows up when dropping the validation item onto a model.
	
	<H3><div align="center" id="boostrap">Bootstrapping</div></H3>
		Bootstrapping creates a training data by random drawing with replacement of compound from the input data set. All compounds that are not part of the created training data set together make up the test set that is used to evaluate the predictive quality after training the model by use of the former one.<br>
		<p>Since the probability of a compound to be chosen for a training set this way is approx. 0.632 and thereby the effective size of the training set (compared to the input data) is reduced, we use the so-called 0.632-estimator to alleviate this bias : <br>
		<div align="center">result for one fold = 0.632*q<sub>test</sub>+0.368*q<sub>train</sub></div>
		where q<sub>test</sub> is the quality of fit to the test set and q<sub>train</sub> the quality of fit to the training set.</p>
		
		<p>Since compounds are drawn with replacement, a high number of very diverse bootstrap samples can be created. This is in contrast to cross validation, where a very high number of folds (e.g. leave-one-out cross-validation) produces very similar training data sets; a property that is usually not desireable.</p>
		
		<p>The average of the results for the k bootstrap samples is then used to describe the predicitve quality of the model.</p>

		<p>The number of desired bootstrap samples can be set in the dialog that shows up when dropping the validation item onto a model.</p>


	<H3><div align="center" id="resp-perm">Response permutation testing</div></H3>
	Boostrapping and cross validation try to evaluate the models predictive quality make no assessment of the statistical significance of this predictive quality.<br>
	<p>It might be the case that the a model seems to able to resonable predict the response value of new data but swapping all response values (and retaining) would lead to similar results.<br>
	Thus, this is what resonse permutation testing does; it randomly permutated the response values, retains the models, applies a cross validation with the permutated data and repeates all this a number of times.</p>

	<p>Comparison of the predictive qualities achieved with permutated response to the one obtained with unpermutated response (see cross-validation <a href="#cv">above</a>) will show you whether your model can achieve significant modelling.<br>
	
	<p>Using this model validation procedure (alongside other ones) is especially important when working with a model that has very many features or a high-dimensional kernel-function or in case of classification problems with a low number of classes.</p>


	<H3><div align="center" id="coeff-stddev">Calculation of regression coefficients' standard deviation</div></H3>
	In order to evaluate the variability of linear regression models, i.e. to see how the obtained regression coefficients deviated between diffent input data sets, the regression coefficients' standard deviation can be calculated. <br>
	This is done by use of bootstrapping; the number of desired samples can be entered into the dialog that shows up when dropping the validation item onto a model.
	<p>After the pipeline has been executed (see <a href="execute.html">here</a>), the average ratio of the obtained standard deviation and the coefficient value of the current model will be displayed. A small value thus indicates relatively steady coefficients, whereas a high value reveals highly variant coefficients.<br>
	Furthermore, the standard deviations of all coefficients can also be plotted (see <a href="analysis.html">here</a>). </p>
	Note that this model validation technique is applicable only to <a href="models.html#model_types">linear regressions models</a>.


	<H3><div align="center" id="nested-val">Nested validation</div></H3>
	<TABLE width="700">
   		<tr>	
		<TD width="370" colspan="1"><IMG src="images/val_nested_scaled.png"></TD>
		<TD>If any feature selection or model-/kernel-parameter optimization has been done, the above procedures alone can not give a realistic estimated of the true predicitve quality of the model.<br>
		The reason for this is that altough during bootstrapping/cross validation many folds consisting of seperate training and testing data set are used to evaluate the model, the <br>
		</TD>
		</tr>
	</TABLE>
	features and/or model-/kernel-parameters that were determined previously, had been obtained eventually using <em>all</em> compounds of the input data set.
	<p>To circumvent this problem, a nested validation pipeline can be created. Therefore the existing pipeline is copied several times and each such nested validation fold is tested on data randomly chosen from the original input data set after being trained on the remaining data (i.e. the one not to be used for testing of this fold).</p>
	The number of desired nested validation folds can be entered in (b), whereas the fraction of the input data that is to be chosen randomly and set aside for validation of each fold can be set in (a).



<!-- begin of footer -->
  
</TABLE>
<hr width=700>
<h2 align="center">
<a href="featureSelection.html"><img src="images/left.png" width="58" height="28" border=0 align="MIDDLE"></a>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="index.html">Index</a> &nbsp;&nbsp;&nbsp;&nbsp;
<a href="executePipeline.html"><img src="images/right.png" width="58" height="28" border=0 align="MIDDLE"></a>
</h2>

</td></tr></table>
  
</body>
</html>

